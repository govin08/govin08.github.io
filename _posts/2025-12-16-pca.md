---
layout: single
title: "Principal Component Analysis"
categories: data-science
tags: [unsupervised learning, principal components, orthogonal diagnoalization, singular value decomposition, partial least regression]
use_math: true
published: true
author_profile: false
toc: true
---

강화학습 글들을 마쳤으니 이제 PCA로 넘어가려 한다.
이 주제에 대해 공부하려고 했었던 이유는, 회사에서 PLS 모델을 자주 쓰는데 이 모델에 대해 내가 잘 알고 있지 못하기 때문이다.
PLS와 많이 연관이 있는 것이 PCA라고 들었고 PCA에 대해 모르는 것은 아니지만 그래도 정확하게 알고 넘어가고 싶다는 생각이 들었다.

워낙 데이터과학에서 자주 쓰이는 알고리즘이고 선형대수에서 바로 이어질 수 있는 기법이기 때문에 설명된 자료들이 많다.
- [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)
- [Pattern Recognition and Machine Learning (12.1절)](https://tjzhifei.github.io/links/PRML.pdf)
- [Principal Component Analysis(2ed, Jolliffe)](https://link.springer.com/book/10.1007/b98835)
- [Principal component analysis with linear algebra](https://www.math.union.edu/~jaureguj/PCA.pdf)

<!-- 가장 먼저는 PCA와 관련된 논문 [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)을 읽었다.
이 논문은 4장까지 읽었는데 4장에서는 행렬의 직교대각화만으로 PCA를 설명했다.
5장은 SVD를 사용하여 더 comprehensive하게 설명하고 있는데 읽진 않았다.
다음으로는 Bishop의 유명한 책 [Pattern Recognition and Machine Learning](https://tjzhifei.github.io/links/PRML.pdf)에서 해당 부분을 찾아보았다.
해당 책의 12.1절에 PCA에 대한 설명이 있는데 조금 읽다가 이해가 안되는 부분이 나와서 읽다가 말았다.
나중에 좀 더 PCA를 이해하고 나면, 혹은 해당 책의 앞부분을 좀 더 보고 나면 이어서 보고 싶다.

그리고는 PCA를 주제로 한 [Jolliffe의 책](https://link.springer.com/book/10.1007/b98835)으로 옮겨타서 읽고 있다. -->
<!-- 도입부에서 다루고 있는 문제가 워낙 간단하고 글을 적고 싶게 만드는 주제라서, 공부를 채 다 끝내기도 전에 블로그 글을 먼저 펼치게 되었다. -->
<!-- 하지만, 당장은 출근해야 하니 나중에 이어가야지. -->

# 1. Preliminaries

## 1.1 dataset, variance and covariance

다음과 같은 데이터셋이 있다고 하자.
세 학생의 물리 성적과 생물 성적을 나열한 표이다.

| students | physics | biology |
|----------|---------|---------|
| A        | 92      | 80      |
| B        | 60      | 30      |
| C        | 100     | 70      |

각 열은 feature를, 각 행은 datapoint를 의미한다.
이 데이터셋이 나타내는 $3\times 2$ 행렬을 $X$라고 하자;

$$
X=
\begin{bmatrix}
92&80\\
60&30\\
100&70
\end{bmatrix}
$$

이 행렬의 $i$번째 행벡터를 $X^{(i)}$로, $j$번째 열벡터를 $X_j$로 표기하자.
또한, $j$번째 feature의 평균을 $\mu_j$라고 하자 ($\mu_1=84$, $\mu_2=60$).
그러면 $X_1$, $X_2$의 분산(variance)은 각각

$$
\begin{align*}
\text{var}(X_1)&=\frac13\left[(92-84)^2+(60-84)^2+(100-84)^2\right]=\frac{896}3\\
\text{var}(X_2)&=\frac13\left[(80-60)^2+(30-60)^2+(70-60)^2\right]=\frac{1400}3
\end{align*}
$$

으로 계산된다.
(여기에서 표본평균과 표본분산이 아닌 모평균과 모분산을 고려하였다.)
$X_1$, $X_2$ 사이의 공분산(covariance)은

$$
\begin{align*}
\text{cov}(X_1,X_2)
&=\frac13\left[(92-84)(80-60)+(60-84)(30-60)+(100-84)(70-60)\right]\\
&=\frac{1040}3
\end{align*}
$$

이다.

일반적으로 $m$개의 feature와 $n$개의 datapoint가 있다고 하면 그 데이터셋은 $n\times m$ 행렬이다.
각 feature들의 평균이 0이라고 하자.
행벡터 $X^{(i)}$는

$$
X^{(i)}=
\begin{bmatrix}
x_{i1}&
\cdots&
x_{im}
\end{bmatrix}
$$

이고, 열벡터 $X_j$는

$$
X_j=
\begin{bmatrix}
x_{1j}\\
\vdots\\
x_{nj}
\end{bmatrix}
$$

이다.
그러면, $j$번째 feature에 대한 분산은

$$
\begin{align*}
\text{var}(X_j)
&=\frac1n\left({x_{1j}}^2+\cdots+{x_{nj}}^2\right)\\
&=\frac1n\sum_{k=1}^n{x_{kj}}^2\\
&=\frac1n{X_j}^TX_j
\tag1
\end{align*}
$$

이고 $i$번째와 $j$번째 feature 사이의 공분산은

$$
\begin{align*}
\text{cov}(X_i,X_j)
&=\frac1n\left(x_{1i}x_{1j}+\cdots+x_{ni}x_{nj}\right)\\
&=\frac1n\sum_{k=1}^nx_{ki}x_{kj}\\
&=\frac1n{X_i}^TX_j
\tag2
\end{align*}
$$

이다.

## 1.2 covariance matrix

이번에는 공분산 행렬을 정의해보려 한다.
이 행렬은 $m\times m$ 행렬로서, $i=j$인 경우에는 그 성분이 $X_i$의 분산이고 $i\ne j$인 경우에는 그 성분이 $X_i$와 $X_j$의 공분산인 행렬을 말한다.
그러면 공분산행렬 $C$는 (1)과 (2)로부터

$$
C = \frac1n X^TX\tag3
$$

으로 정의된다.
혹은 다음과 같이 행벡터로서 정의될 수도 있다.

$$
\begin{align*}
C
&=\frac1n\sum_{k=1}^n
\begin{bmatrix}x_{k1}\\\vdots\\x_{km}\end{bmatrix}
\begin{bmatrix}x_{k1}&\cdots&x_{km}\end{bmatrix}\\
&=\frac1n\sum_{k=1}^n
{X^{(k)}}^TX^{(k)}
\end{align*}
$$

당연히, 행렬 $C$는 대칭행렬(symmetric matrix)이다.

## 1.3 Some matrix calculations

$m$차원 벡터 $v=\begin{bmatrix}v_1&\cdots&v_m\end{bmatrix}^T$에 대하여 $v$의 norm은

$$\lVert v\rVert =\sqrt{v^Tv}=\sqrt{v_1^2+\cdots+v_m^2}$$

이다.
$v^Tv$를 $v$에 대해 미분하면, 즉 gradient를 구하면

$$
\frac{\partial}{\partial v}v^Tv
=\begin{bmatrix}
2v_1\\\vdots\\2v_m
\end{bmatrix}
=2v
\tag4
$$

이다.
$m\times m$ 행렬 $A$에 대하여 $A$와 $v$의 이차형식(quadratic form)은 $v^TAv$으로 정의된다.
이것을 계산하면

$$
\begin{align*}
v^TAv
&=
\begin{bmatrix}v_1&\cdots&v_m\end{bmatrix}
\begin{bmatrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{m1}&\cdots&a_{mm}
\end{bmatrix}
\begin{bmatrix}v_1\\\vdots\\v_m\end{bmatrix}\\
&=
\begin{bmatrix}
\sum_{i=1}^ma_{i1}v_i&\cdots\sum_{i=1}^ma_{im}v_i
\end{bmatrix}
\begin{bmatrix}v_1\\\vdots\\v_m\end{bmatrix}\\
&=\sum_{i=1}^m\sum_{j=1}^ma_{ij}v_iv_j
\end{align*}
$$

이것을 $v$의 한 성분 $v_k$에 대하여 편미분하면

$$
\begin{align*}
\frac\partial{\partial v_k}a_{kk}v_kv_k&=2a_{kk}v_k\\
\frac\partial{\partial v_k}a_{ik}v_iv_k&=a_{ik}v_i (i\ne k)\\
\frac\partial{\partial v_k}a_{kj}v_kv_j&=a_{kj}v_j (j\ne k)
\end{align*}
$$

로부터

$$
\frac\partial{\partial v_k}v^TAv=\sum_{i=1}^ma_{ik}v_i+\sum_{j=1}^ma_{kj}v_j
$$

이다.
만약 $A$가 대칭행렬이면

$$
\frac\partial{\partial v_k}v^TAv=2\sum_{j=1}^ma_{kj}v_j
$$

이다.
$v^TAv$를 $v$에 대해 미분하면, 즉 gradient를 구하면,

$$
\begin{align*}
\frac\partial{\partial v}v^TAv
&=2\begin{bmatrix}
\sum_{j=1}^ma_{1j}v_j\\
\vdots\\
\sum_{j=1}^ma_{mh}v_j
\end{bmatrix}\\
&=2\begin{bmatrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{m1}&\cdots&a_{mm}\\
\end{bmatrix}
\begin{bmatrix}
v_1\\\vdots\\v_m
\end{bmatrix}\\
&=2Av
\tag5
\end{align*}
$$

<!-- ## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

# -->
<!-- 
# 1. Preliminaries

## 1.1 Motivation : Equivalences
The equivalence between maximizing variance and mimimizing distances.

## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

# -->