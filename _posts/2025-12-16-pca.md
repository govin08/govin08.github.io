---
layout: single
title: "Principal Component Analysis"
categories: data-science
tags: [unsupervised learning, principal components, orthogonal diagnoalization, singular value decomposition, partial least regression]
use_math: true
published: true
author_profile: false
toc: true
---

강화학습 글들을 마쳤으니 이제 PCA로 넘어가려 한다.
이 주제에 대해 공부하려고 했었던 이유는, 회사에서 PLS 모델을 자주 쓰는데 이 모델에 대해 내가 잘 알고 있지 못하기 때문이다.
PLS와 많이 연관이 있는 것이 PCA라고 들었고 PCA에 대해 모르는 것은 아니지만 그래도 정확하게 알고 넘어가고 싶다는 생각이 들었다.

데이터과학에서 자주 쓰이는 알고리즘이고 선형대수에서 바로 이어질 수 있는 기법이기 때문에 설명된 자료들이 워낙 많다.
- [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)
- [Pattern Recognition and Machine Learning (12.1절)](https://tjzhifei.github.io/links/PRML.pdf)
- [Principal Component Analysis(2ed, Jolliffe)](https://link.springer.com/book/10.1007/b98835)
- [Principal component analysis with linear algebra](https://www.math.union.edu/~jaureguj/PCA.pdf)

<!-- 가장 먼저는 PCA와 관련된 논문 [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)을 읽었다.
이 논문은 4장까지 읽었는데 4장에서는 행렬의 직교대각화만으로 PCA를 설명했다.
5장은 SVD를 사용하여 더 comprehensive하게 설명하고 있는데 읽진 않았다.
다음으로는 Bishop의 유명한 책 [Pattern Recognition and Machine Learning](https://tjzhifei.github.io/links/PRML.pdf)에서 해당 부분을 찾아보았다.
해당 책의 12.1절에 PCA에 대한 설명이 있는데 조금 읽다가 이해가 안되는 부분이 나와서 읽다가 말았다.
나중에 좀 더 PCA를 이해하고 나면, 혹은 해당 책의 앞부분을 좀 더 보고 나면 이어서 보고 싶다.

그리고는 PCA를 주제로 한 [Jolliffe의 책](https://link.springer.com/book/10.1007/b98835)으로 옮겨타서 읽고 있다. -->
<!-- 도입부에서 다루고 있는 문제가 워낙 간단하고 글을 적고 싶게 만드는 주제라서, 공부를 채 다 끝내기도 전에 블로그 글을 먼저 펼치게 되었다. -->
<!-- 하지만, 당장은 출근해야 하니 나중에 이어가야지. -->

# 1. Preliminaries

## 1.1 dataset, variance and covariance

다음과 같은 데이터셋이 있다고 하자.
세 학생의 물리 성적과 생물 성적을 나열한 표이다.

| students | physics | biology |
|----------|---------|---------|
| A        | 92      | 80      |
| B        | 60      | 30      |
| C        | 100     | 70      |

각 열은 feature를, 각 행은 datapoint를 의미한다.
이 데이터셋이 나타내는 $n\times m$ 행렬을 $X$라고 하자;

$$
X=
\begin{bmatrix}
92&80\\
60&30\\
100&70
\end{bmatrix}
$$

이 행렬의 $i$번째 열벡터를 $X_i$, $j$번째 행벡터를 $X^{(j)}$로 표기하자.
또한, $X_i$의 평균을 $\mu_i$라고 하자 ($\mu_1=84$, $\mu_2=60$).
그러면 $X_1$, $X_2$의 분산(variance)은 각각

$$
\begin{align*}
\text{var}(X_1)&=\frac13\left[(92-84)^2+(60-84)^2+(100-84)^2\right]=\frac{896}3\\
\text{var}(X_2)&=\frac13\left[(80-60)^2+(30-60)^2+(70-60)^2\right]=\frac{896}3
\end{align*}
$$

## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

#
<!-- 
# 1. Preliminaries

## 1.1 Motivation : Equivalences
The equivalence between maximizing variance and mimimizing distances.

## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

# -->