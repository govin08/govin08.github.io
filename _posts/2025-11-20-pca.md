---
layout: single
title: "Principal Component Analysis"
categories: data-science
tags: [unsupervised learning, principal components, orthogonal diagnoalization, singular value decomposition, partial least regression]
use_math: true
published: false
author_profile: false
toc: true
---

강화학습 글들을 마쳤으니 이제 PCA로 넘어가려 한다.
이 주제에 대해 공부하려고 했었던 이유는, 회사에서 PLS 모델을 자주 쓰는데 이 모델에 대해 내가 잘 알고 있지 못하기 때문이다.
PLS와 많이 연관이 있는 것이 PCA라고 들었고 PCA에 대해 모르는 것은 아니지만 그래도 정확하게 알고 넘어가고 싶다는 생각이 들었다.

워낙 많이 쓰이는 알고리즘이고 선형대수에서 바로 이어질 수 있는 기법이기 때문에 설명된 자료들이 워낙 많다.
가장 먼저는 PCA와 관련된 논문 [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)을 읽었다.
이 논문은 4장까지 읽었는데 4장에서는 행렬의 직교대각화만으로 PCA를 설명했다.
5장은 SVD를 사용하여 더 comprehensive하게 설명하고 있는데 읽진 않았다.
다음으로는 Bishop의 유명한 책 [Pattern Recognition and Machine Learning](https://tjzhifei.github.io/links/PRML.pdf)에서 해당 부분을 찾아보았다.
해당 책의 12.1절에 PCA에 대한 설명이 있는데 조금 읽다가 이해가 안되는 부분이 나와서 읽다가 말았다.
나중에 좀 더 PCA를 이해하고 나면, 혹은 해당 책의 앞부분을 좀 더 보고 나면 이어서 보고 싶다.

그리고는 PCA를 주제로 한 [Jolliffe의 책](https://link.springer.com/book/10.1007/b98835)으로 옮겨타서 읽고 있다.
도입부에서 다루고 있는 문제가 워낙 간단하고 글을 적고 싶게 만드는 주제라서, 공부를 채 다 끝내기도 전에 블로그 글을 먼저 펼치게 되었다.
하지만, 당장은 출근해야 하니 나중에 이어가야지.

# 1. Preliminaries

## 1.1 Motivation : Equivalences
The equivalence between maximizing variance and mimimizing distances.

## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

#