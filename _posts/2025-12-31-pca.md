---
layout: single
title: "Principal Component Analysis"
categories: data-science
tags: [unsupervised learning, principal components, linear algebra, orthogonal diagonalization, singular value decomposition]
use_math: true
published: true
author_profile: false
toc: true
---

강화학습 글들을 마쳤으니 이제 PCA로 넘어가려 한다.
이 주제에 대해 공부하려고 했었던 이유는, 회사에서 PLS 모델을 자주 쓰는데 이 모델에 대해 내가 잘 알고 있지 못하기 때문이다.
PLS와 많이 연관이 있는 것이 PCA라고 들었고 PCA에 대해 모르는 것은 아니지만 그래도 정확하게 알고 넘어가고 싶다는 생각이 들었다.

워낙 데이터과학에서 자주 쓰이는 알고리즘이고 선형대수에서 바로 이어질 수 있는 기법이기 때문에 설명된 자료들이 많다.
- [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)
- [Pattern Recognition and Machine Learning (12.1절)](https://tjzhifei.github.io/links/PRML.pdf)
- [Principal Component Analysis(2ed, Jolliffe)](https://link.springer.com/book/10.1007/b98835)
- [Principal component analysis with linear algebra](https://www.math.union.edu/~jaureguj/PCA.pdf)

<!-- 가장 먼저는 PCA와 관련된 논문 [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)을 읽었다.
이 논문은 4장까지 읽었는데 4장에서는 행렬의 직교대각화만으로 PCA를 설명했다.
5장은 SVD를 사용하여 더 comprehensive하게 설명하고 있는데 읽진 않았다.
다음으로는 Bishop의 유명한 책 [Pattern Recognition and Machine Learning](https://tjzhifei.github.io/links/PRML.pdf)에서 해당 부분을 찾아보았다.
해당 책의 12.1절에 PCA에 대한 설명이 있는데 조금 읽다가 이해가 안되는 부분이 나와서 읽다가 말았다.
나중에 좀 더 PCA를 이해하고 나면, 혹은 해당 책의 앞부분을 좀 더 보고 나면 이어서 보고 싶다.

그리고는 PCA를 주제로 한 [Jolliffe의 책](https://link.springer.com/book/10.1007/b98835)으로 옮겨타서 읽고 있다. -->
<!-- 도입부에서 다루고 있는 문제가 워낙 간단하고 글을 적고 싶게 만드는 주제라서, 공부를 채 다 끝내기도 전에 블로그 글을 먼저 펼치게 되었다. -->
<!-- 하지만, 당장은 출근해야 하니 나중에 이어가야지. -->

# 1. motivation

2차원 평면 위의 $n$개의 점들 $X^{(1)}$, $\cdots$ $X^{(n)}$을 생각하자.

![1-1]({{site.url}}\images\2025-12-31-pca\1-1.png){: .img-50-center}

이 $n$개의 점들을 가장 잘 설명할 수 있는 직선 $l$을 그어보자.
'잘 설명할 수 있다'는 것의 의미는 

> (a) 각 점에서 직선 $l$까지의 거리의 제곱의 평균이 최소가 되는 경우

를 의미한다.
(a)는 다음 그림에서 빨간 선들의 길이의 제곱의 평균이 최소가 되는 경우를 뜻한다.

![1-2]({{site.url}}\images\2025-12-31-pca\1-2.png){: .img-50-center}

일반성을 잃지 않고 $n$개의 점들의 무게중심이 원점이라고 하자.
원점을 지나는 직선은 방향벡터로 결정되므로, (a)는 빨간 선들의 길이의 제곱의 평균이 최소가 되는 방향벡터 $v$를 구하는 문제가 된다.

조건 (b)를

> (b) 직선 $l$ 방향의 분산이 최대가 되는 경우

라고 하자.
다시 말해, 각 점들의 직선 $l$로의 수선의 발들의 원점까지의 거리의 제곱의 평균이 최대가 되는 경우를 뜻한다.
그러면 (a)와 (b)는 동치이다.

그 이유를 설명하기 위해 다음 그림과 같이 한 점 $X^{(i)}$를 생각하면

![1-3]({{site.url}}\images\2025-12-31-pca\1-3.png){: .img-50-center}

피타고라스정리로부터 빨간 선분의 길이의 제곱과 파란 선분의 길이의 제곱의 합은 $\overline{OX^{(i)}}^2$ 으로 일정하다는 사실을 사용할 수 있다.

즉, 점 $X^{(i)}$에서 $l$로의 수선의 발은 $H^{(i)}=\left(X^{(i)}\cdot v\right)v$라고 할 수 있는데 이때, $\lVert H^{(i)}\rVert=X^{(i)}\cdot v$이고 (a)에서 최소화해야 하는 값 $A$는

$$A=\frac1n\sum_{i=1}^n\lVert X^{(i)}-H^{(i)}\rVert^2$$

이고 (b)에서 최대화해야 하는 값 $B$는

$$B=\frac1n\sum_{i=1}^n\lVert H^{(i)}\rVert^2$$

이다.
그러면 피타고라스 정리로부터

$$
\begin{align*}
A+B
&=\frac1n\sum_{i=1}^n
\left(\lVert X^{(i)}-H^{(i)}\rVert^2+\lVert H^{(i)}\rVert^2\right)\\
&=\frac1n\sum_{i=1}^n
\lVert X^{(i)}\rVert^2
\end{align*}
$$

이고 $A+B$의 값이 $v$에 관계없이 일정하다.
따라서 $A$를 최소화하는 것과 $B$를 최대화하는 것은 서로 같은 말이 된다.
$v$를 바꿔가면서 $A$(MSE)와 $B$(variance)를 표시해보면, 정말로 MSE가 최소가 될 때 variance가 최대가 됨을 알 수 있다.

![1-4]({{site.url}}\images\2025-12-31-pca\1-4.gif){: .img-50-center}

<video autoplay loop muted playsinline class="img-50-center">
  <source src="{{ site.url }}/images/2025-12-31-pca/1-4.mp4" type="video/mp4">
</video>

이와 같이 (a) 또는 (b) 조건을 만족시키는 방향 $v$를 주성분이라고 부른다.

이것은 $n$개의 2차원 데이터에서 주성분 1개를 찾는 과정에 해당한다.
[Pattern Recognition and Machine Learning](https://tjzhifei.github.io/links/PRML.pdf)의 12.1.1, 12.1.2에서는 $N$개의 $D$차원 데이터에서 주성분 $M$개를 찾는 과정에서 Variance를 최대화하는 것이 MSE를 최소화하는 것과 동등함을 설명하고 있다.

<!-- # 2.

$m$개의 feature들을 일차결합하여 새로운 성분 $Xv$을 만들되, 의미있는 성분을 만들어보려 한다 (단, $v\in\mathbb R^m$).
이때 '의미있는 성분'이라 함은, 주어진 데이터셋을 가장 잘 설명하는 성분을 말하며, 의미있는 성분을 찾는다는 것은 다음 둘 중 하나의 조건을 만족시키는 방향 $v$를 찾는 것이다.

(a) $v$ 방향으로의 분산이 최대가 된다.

(b) $v$ 방향의 직선까지의 거리가 최소가 된다.

재밌는 사실은 (a)와 (b)가 동등한 조건이라는 사실이다.

그림을 통해 보면 명확히 보일 것이다.
$m=2$인 상황을 가정하자.
$n=13$이므로 2차원 평면에 13개의 점이 찍혀있다.
13개 점의 무게중심은 원점이다.

![points]({{site.url}}\images\2025-12-16-pca\points.png){: .img-50-center}

 -->


# 2. Preliminaries

## 2.1 dataset, variance, covariance

다음과 같은 데이터셋이 있다고 하자.
세 학생의 물리 성적과 생물 성적을 나열한 표이다.

| students | physics | biology |
|----------|---------|---------|
| A        | 92      | 80      |
| B        | 60      | 30      |
| C        | 100     | 70      |

각 열은 feature를, 각 행은 datapoint를 의미한다.
이 데이터셋이 나타내는 $3\times 2$ 행렬을 $X$라고 하자;

$$
X=
\begin{bmatrix}
92&80\\
60&30\\
100&70
\end{bmatrix}
$$

이 행렬의 $i$번째 행벡터를 $X^{(i)}$로, $j$번째 열벡터를 $X_j$로 표기하자.
또한, $j$번째 feature의 평균을 $\mu_j$라고 하자 ($\mu_1=84$, $\mu_2=60$).
그러면 $X_1$, $X_2$의 분산(variance)은 각각

$$
\begin{align*}
\text{var}(X_1)&=\frac13\left[(92-84)^2+(60-84)^2+(100-84)^2\right]=\frac{896}3\\
\text{var}(X_2)&=\frac13\left[(80-60)^2+(30-60)^2+(70-60)^2\right]=\frac{1400}3
\end{align*}
$$

으로 계산된다.
(여기에서 표본평균과 표본분산이 아닌 모평균과 모분산을 사용하였다.)
$X_1$, $X_2$ 사이의 공분산(covariance)은

$$
\begin{align*}
\text{cov}(X_1,X_2)
&=\frac13\left[(92-84)(80-60)+(60-84)(30-60)+(100-84)(70-60)\right]\\
&=\frac{1040}3
\end{align*}
$$

이다.

일반적으로 $m$개의 feature와 $n$개의 datapoint가 있다고 하면 그 데이터셋은 $n\times m$ 행렬이다.
각 feature들의 평균이 0이라고 하자.
행벡터 $X^{(i)}$는

$$
X^{(i)}=
\begin{bmatrix}
x_{i1}&
\cdots&
x_{im}
\end{bmatrix}
$$

이고, 열벡터 $X_j$는

$$
X_j=
\begin{bmatrix}
x_{1j}\\
\vdots\\
x_{nj}
\end{bmatrix}
$$

이다.
그러면, $j$번째 feature에 대한 분산은

$$
\begin{align*}
\text{var}(X_j)
&=\frac1n\left({x_{1j}}^2+\cdots+{x_{nj}}^2\right)\\
&=\frac1n\sum_{k=1}^n{x_{kj}}^2\\
&=\frac1n{X_j}^TX_j
\tag1
\end{align*}
$$

이고 $i$번째와 $j$번째 feature 사이의 공분산은

$$
\begin{align*}
\text{cov}(X_i,X_j)
&=\frac1n\left(x_{1i}x_{1j}+\cdots+x_{ni}x_{nj}\right)\\
&=\frac1n\sum_{k=1}^nx_{ki}x_{kj}\\
&=\frac1n{X_i}^TX_j
\tag2
\end{align*}
$$

이다.

## 2.2 covariance matrix

이번에는 공분산 행렬을 정의해보려 한다.
이 행렬은 $m\times m$ 행렬로서, $i=j$인 경우에는 그 성분이 $X_i$의 분산이고 $i\ne j$인 경우에는 그 성분이 $X_i$와 $X_j$의 공분산인 행렬을 말한다.
그러면 공분산행렬 $C$는 (1)과 (2)로부터

$$
C = \frac1n X^TX\tag3
$$

으로 정의된다.
혹은 다음과 같이 행벡터로서 정의될 수도 있다.

$$
\begin{align*}
C
&=\frac1n\sum_{k=1}^n
\begin{bmatrix}x_{k1}\\\vdots\\x_{km}\end{bmatrix}
\begin{bmatrix}x_{k1}&\cdots&x_{km}\end{bmatrix}\\
&=\frac1n\sum_{k=1}^n
{X^{(k)}}^TX^{(k)}
\end{align*}
$$

당연히, 행렬 $C$는 대칭행렬(symmetric matrix)이다.

## 2.3 some matrix calculations

$m$차원 벡터 $v=\begin{bmatrix}v_1&\cdots&v_m\end{bmatrix}^T$에 대하여 $v$의 norm은

$$\lVert v\rVert =\sqrt{v^Tv}=\sqrt{v_1^2+\cdots+v_m^2}$$

이다.
$v^Tv$를 $v$에 대해 미분하면, 즉 gradient를 구하면

$$
\frac{\partial}{\partial v}v^Tv
=\begin{bmatrix}
2v_1\\\vdots\\2v_m
\end{bmatrix}
=2v
\tag4
$$

이다.
$m\times m$ 행렬 $A$에 대하여 $A$와 $v$의 이차형식(quadratic form)은 $v^TAv$으로 정의된다.
이것을 계산하면

$$
\begin{align*}
v^TAv
&=
\begin{bmatrix}v_1&\cdots&v_m\end{bmatrix}
\begin{bmatrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{m1}&\cdots&a_{mm}
\end{bmatrix}
\begin{bmatrix}v_1\\\vdots\\v_m\end{bmatrix}\\
&=
\begin{bmatrix}
\sum_{i=1}^ma_{i1}v_i&\cdots\sum_{i=1}^ma_{im}v_i
\end{bmatrix}
\begin{bmatrix}v_1\\\vdots\\v_m\end{bmatrix}\\
&=\sum_{i=1}^m\sum_{j=1}^ma_{ij}v_iv_j
\end{align*}
$$

이것을 $v$의 한 성분 $v_k$에 대하여 편미분하면

$$
\begin{align*}
\frac\partial{\partial v_k}a_{kk}v_kv_k&=2a_{kk}v_k\\
\frac\partial{\partial v_k}a_{ik}v_iv_k&=a_{ik}v_i (i\ne k)\\
\frac\partial{\partial v_k}a_{kj}v_kv_j&=a_{kj}v_j (j\ne k)
\end{align*}
$$

로부터

$$
\frac\partial{\partial v_k}v^TAv=\sum_{i=1}^ma_{ik}v_i+\sum_{j=1}^ma_{kj}v_j
$$

이다.
만약 $A$가 대칭행렬이면

$$
\frac\partial{\partial v_k}v^TAv=2\sum_{j=1}^ma_{kj}v_j
$$

이다.
$v^TAv$를 $v$에 대해 미분하면, 즉 gradient를 구하면,

$$
\begin{align*}
\frac\partial{\partial v}v^TAv
&=2\begin{bmatrix}
\sum_{j=1}^ma_{1j}v_j\\
\vdots\\
\sum_{j=1}^ma_{mh}v_j
\end{bmatrix}\\
&=2\begin{bmatrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{m1}&\cdots&a_{mm}\\
\end{bmatrix}
\begin{bmatrix}
v_1\\\vdots\\v_m
\end{bmatrix}\\
&=2Av
\tag5
\end{align*}
$$

## 2.4 real symmetric matrices

만약 $A$가 real symmetric이면 (실수로 이루어진 대칭행렬이면) 다음이 성립한다.
- (a) $A$의 모든 eigenvalue는 실수이다
- (b) $A=BDB^{-1}$를 만족시키는 대각행렬 $D$와 직교행렬 $B$가 존재한다. (직교대각화 가능)

이에 관한 설명은 [행렬의 직교대각화](https://govin08.github.io/mathematics/diagonalization/) 포스팅에서 정리 19와 성질 20으로 갈음한다.

## 2.5 Lagrange multiplier method

두 함수 $f:\mathbb R^n\to\mathbb R$와 $g:\mathbb R^n\to\mathbb R^m$에 대하여 ($f,g\in C^1$) 최적화 문제

<div class="notice--info">
$$\text{Maximize }f(\boldsymbol x)\text{ subject to }g(\boldsymbol x)=\boldsymbol 0.$$
</div>

는 함수 $\mathcal L:\mathbb R^n\times\mathbb R^m\to\mathbb R$을

$$
\mathcal L\left(\boldsymbol x,\boldsymbol\lambda\right)=f(\boldsymbol x)+\boldsymbol\lambda^Tg(\boldsymbol x)
$$

로 정의하여 ($\boldsymbol\lambda\in\mathbb R^m$)

<div class="notice--info">
$$\text{Maximize }\mathcal L\left(\boldsymbol x,\boldsymbol\lambda\right).$$
</div>

를 풀어도 된다.
이에 관해서는 [직전 포스트](https://govin08.github.io/mathematics/lagrangian/)에 자세히 설명해놓았다.


## 2.2 definition of PCs

## 2.4 PCA and orthogonal diagonalization

## 2.5 Singluar value decomposition

## 2.6 PCA and SVD

<!-- 
# 1. Preliminaries

## 1.1 Motivation : Equivalences
The equivalence between maximizing variance and mimimizing distances.

## 1.2 Covariance matrix of a dataset

## 1.3 Some matrix Calculations

## 1.4 Lagrange multiplier

# 2. PCA

## 2.1 Problem formulation

## 2.2 Orthogonal diagonalization

## 2.3 PCA using orthogonal diagonalization

## 2.4 Singluar value decomposition

## 2.5 PCA using SVD

# 3. PLS

# -->