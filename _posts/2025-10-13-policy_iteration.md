---
layout: single
title: "(Sutton, 4.2, 4.3ì ˆ) Policy Iteration"
categories: data-science
tags: [reinforcement learing, dynamic programming, policy improvement theorem, policy iteration]
use_math: true
published: true
author_profile: false
toc: true
---

ë” ì‹œê°„ì´ ì§€ë‚˜ê¸° ì „ì— dynamic programming í¬ìŠ¤íŒ…ì„ ëë‚´ê³  ì‹¶ì€ ë§ˆìŒì´ ìƒê²¼ë‹¤.
ë‹¤ë¥¸ ì£¼ì œë“¤ë„ ê³µë¶€í•˜ì—¬ í¬ìŠ¤íŒ…ì„ ë‚¨ê¸°ê³  ì‹¶ì€ë° DPë¥¼ ëë‚´ì§€ ì•Šê³  ë‹¤ë¥¸ ê²ƒì„ ì“°ê¸°ëŠ” ì‹«ê¸° ë•Œë¬¸ì´ë‹¤.
ê·¸ëŸ¬ë‹ˆê¹Œ ì¼ì¢…ì˜ ì˜ë¬´ê°ì—ì„œ ì´ ê¸€ì„ ì“°ê³  ìˆë‹¤.
ë‹¹ì¥ ì´ì „ë¶€í„° PCAì™€ PLSì— ëŒ€í•´ ê³µë¶€í•˜ê³  ì‹¶ì—ˆê³  ì–¼ë§ˆ ì „ì—ëŠ” game theoryë‚˜ control theoryì— ì†ì„ ëŒˆê¹Œë„ ìƒê°í–ˆì—ˆëŠ”ë°, ì˜¤ëŠ˜ì€ MPCì™€ LQRì„ ë°°ì›Œì•¼ í•  í•„ìš”ê°€ ìƒê¸´ ê²ƒì´ë‹¤.
ê·¸ëŸ¬ë‹ˆ DPëŠ” ë¹ ë¥´ê²Œ ê³µë¶€í•˜ì—¬ ì¹˜ì›Œë²„ë¦¬ì.

ê·¸ë¦¬ê³  ì‚¬ì‹¤ ê¸€ì„ ì“¸ ì¤€ë¹„ê°€ ë˜ì–´ìˆë‹¤ê³  ìƒê°í•œë‹¤.
[ì´ì „ ê¸€](https://govin08.github.io/data-science/policy_evaluation/)ì„ ì“°ê³  ë‚˜ì„œ ê°„ê°„ì´ 4.2ì ˆì„ ë³´ì•˜ê³  ì–´ëŠ ì •ë„ ì´í•´ëŠ” í–ˆë˜ í„°ì˜€ë‹¤.
ì±…ì˜ ë‚´ìš©ëŒ€ë¡œ ì­‰ ë”°ë¼ê°€ë³¼ê¹Œ.

í•˜ì§€ë§Œ ê¸€ì“°ëŠ” ê²ƒì´ ì‰½ì§€ëŠ” ì•Šì„ ê²ƒ ê°™ë‹¤.
ì‚¬ì‹¤ìƒ Suttonì˜ ì±…ì— ì“°ì¸ ê²ƒë³´ë‹¤ ë” ê°„ê²°í•˜ê³  ì˜ ì“¸ ìˆ˜ëŠ” ì—†ì„í…Œë‹ˆ, ê¸€ì„ ì“°ê³  ë‚˜ì„œë„ ë‚˜ì¡°ì°¨ë„ ë‚´ ë¸”ë¡œê·¸ ê¸€ì„ ë³´ê¸°ë³´ë‹¤ëŠ” Suttonì˜ ì±…ì„ ë³¼ ê²ƒ ê°™ë‹¤.
ê²Œë‹¤ê°€ ë‹¹ì´ˆì˜ ëª©ì ì€ policy evaluation ì¦ëª…ì´ì—ˆìœ¼ë‹ˆ ì›ë˜ì˜ ë™ê¸°ë¶€ì—¬ëŠ” ë‹¬ì„±ëœ ì…ˆì´ë‹¤.
ê·¸ë˜ë„ DPë¥¼ ì‹œì‘í–ˆìœ¼ë‹ˆ ëì„ ë³´ê³ ëŠ” ì‹¶ë‹¤.
Suttonì˜ ì±…ì„ ë¹„ìŠ·í•˜ê²Œ ì“°ë˜ ì„¤ëª…ì„ ì¢€ ì˜ ì ì–´ë³´ëŠ” ì‹ìœ¼ë¡œ í•˜ì.

## 4.2 Policy Improvement

<!-- 4.2ì ˆì€ policy improvement theoremìœ¼ë¡œë¶€í„° ì‹œì‘í•œë‹¤. -->

policy evaluationì„ í†µí•´ $v_\pi$ ë˜ëŠ” $q_\pi$ë¥¼ ì¶”ì •í•œ ê²ƒì€ ë” ë‚˜ì€ ì •ì±…ì„ ì°¾ê¸° ìœ„í•¨ì´ë¼ê³  ë´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤.
ì‹¤ì œë¡œ Q í•¨ìˆ˜ë¥¼ ì–»ì–´ëƒˆë‹¤ë©´ ê·¸ Qí•¨ìˆ˜ì— ì˜ê±°í•˜ì—¬ ê°€ì¥ ê´œì°®ì€ ì •ì±…, ì¦‰ ì£¼ì–´ì§„ $s$ì— ëŒ€í•˜ì—¬ $Q(s,a)$ê°€ ìµœëŒ€ê°€ ë˜ëŠ” $a$ë¥¼ ì„ íƒí•˜ê²Œ í•˜ëŠ” ì •ì±…ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆë‹¤.
ì´ê²ƒì„ greedy policyë¼ê³  í•œë‹¤.
ë¬¼ë¡  Q í•¨ìˆ˜ê°€ ì •í™•í•˜ë‹¤ëŠ” ë³´ì¥ì´ ì—†ê¸° ë•Œë¬¸ì— greedy policyê°€ optimal policyë¼ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, ê·¸ë˜ë„ ì£¼ì–´ì§„ $Q$ì— ëŒ€í•œ ìµœì„ ì˜ ì •ì±…ì´ê² ë‹¤.

Suttonì˜ 4.2ì ˆì€ ê¸°ì¡´ì˜ ì •ì±… $\pi$ì— ë¹„í•´ $Q$ì˜ ê´€ì ì—ì„œ ë” ë‚˜ì€ ì •ì±… $\pi'$ì´ ì •ë§ë¡œ ë‚˜ì€ ì •ì±…ì´ë¼ëŠ” ì •ë¦¬ë¡œë¶€í„° ì‹œì‘í•œë‹¤.
ì¦‰, ìƒíƒœ $s$ì— ëŒ€í•˜ì—¬ ì •ì±… $\pi$ë¥¼ ë”°ë¼ê°”ì„ ë•Œì˜ ê°€ì¹˜ $v_\pi(s)$ì— ë¹„í•´, ìƒˆë¡œìš´ í–‰ë™ $a=\pi'(s)\ne\pi(s)$ì„ ì„ íƒí•œ í›„ ì •ì±… $\pi$ë¥¼ ë”°ë¼ê°ˆ ë•Œì˜ ê°€ì¹˜ê°€ ë” í¬ë‹¤ë©´, ìƒˆë¡œìš´ ì •ì±… $\pi'$ê°€ ê¸°ì¡´ì˜ ì •ì±… $\pi$ë³´ë‹¤ ë‚«ë‹¤ëŠ” ì •ë¦¬ì´ë‹¤.

<div class="notice--info">
<b> policy improvement theorem (ì •ì±… ê°œì„  ì •ë¦¬) </b> <br>
deterministic policy $\pi$, $\pi'$ì— ëŒ€í•˜ì—¬ ëª¨ë“  $s\in\mathcal S$
$$q_\pi(s,\pi'(s))\ge v_\pi(s)\tag{4.7}$$
ë¼ë©´ ëª¨ë“  $s\in\mathcal S$ì— ëŒ€í•˜ì—¬
$$v_{\pi'}(s)\ge v_\pi(s)\tag{4.8}$$
ê°€ ì„±ë¦½í•œë‹¤.
</div>

ì´ì— ëŒ€í•œ ì¦ëª…ì€ êµ‰ì¥íˆ straightforwardí•œë°, ì±…ì— ìˆëŠ” ê·¸ëŒ€ë¡œ ë”°ë¼ ì¨ë³´ê² ë‹¤.
ì—„ë°€í•˜ê²Œ ë“¤ì–´ê°€ë ¤ë©´ í™•ë¥ ë³€ìˆ˜ë“¤ì˜ ê¸‰ìˆ˜ë¥¼ ì–´ë–»ê²Œ ì •ì˜í• ì§€ í•˜ëŠ” ì–´ë ¤ìš´ ë¬¸ì œë¥¼ ë‹¤ë¤„ì•¼ í•˜ê² ì§€ë§Œ ê·¸ëŸ° ê²ƒì˜ ì„¸ë¶€ëŠ” í”¼í•˜ê³ , ë‹¤ë§Œ ê°ê°ì˜ ìŠ¤í…ì´ ì–´ë–»ê²Œ ì •ë‹¹í™”ë  ìˆ˜ ìˆëŠ”ì§€ë§Œ ë³´ì.

$$
\begin{align*}
v_\pi(s)
\le&q_\pi(s, \pi'(s))\\
=&\mathbb E\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi'(s)\right]\\
=&\mathbb E_{\pi'}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]\\
\le&\mathbb E_{\pi'}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1})\vert S_t=s\right]\\
=&\mathbb E_{\pi'}\left[R_{t+1}+\gamma\mathbb E_{\pi'}\left[R_{t+2}+\gamma v_\pi(S_{t+2})\vert S_{t+1},A_{t+1}=\pi'(S_{t+1})\right] \vert S_t=s\right]\\
=&\mathbb E_{\pi'}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})\vert S_t=s\right]\\
\le&\mathbb E_{\pi'}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})\vert S_t=s\right]\\
&\vdots\\
\le&\mathbb E_{\pi'}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\cdots\vert S_t=s\right]\\
=v_{\pi'}(s)
\end{align*}
$$

ì²«ì§¸ì¤„ì€ (4.7)ì„, ë‘˜ì§¸ì¤„ì€ ì´ì „ í¬ìŠ¤íŠ¸ì˜ (4.3)ì˜ ì¦ëª…ê³¼ ë¹„ìŠ·í•˜ê²Œ ì–»ì–´ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì…‹ì§¸ì¤„ë„ ê¸°í˜¸ì˜ ì •ì˜ì— ì˜í•´ ë‹¹ì—°í•˜ë‹¤.
ë„·ì§¸ì¤„ì€ (4.7)ì„ ë‹¤ì‹œ ì‚¬ìš©í•œ ê²ƒì´ê³  ë‹¤ì„¯ì§¸ì¤„ì€ ë‘˜ì§¸ì¤„ê³¼ ë¹„ìŠ·í•˜ë‹¤.
ë‹¤ì„¯ì§¸ì¤„ì— ìƒˆë¡œ ìƒê¸´ expectationì€ ì–´ë–¤ subscriptë„ ì—†ì´ $\mathbb E$ë¡œ ì“°ì¼ ìˆ˜ ìˆê² ì§€ë§Œ $\mathbb E_{\pi'}$ë¼ê³  ì“°ì—¬ë„ ë¬´ë°©í•˜ë‹¤.
ê·¸ë¦¬ê³  $\mathbb E_{\pi'}$ìœ¼ë¡œ ì í˜”ê¸° ë•Œë¬¸ì— ë‹¤ìŒ ì—¬ì„¯ë²ˆì§¸ ì‹ì´, Markov propertyì™€ ë”ë¶ˆì–´ ì„±ë¦½í•˜ê²Œ ëœë‹¤.
ì´ì œ, ì…‹ì§¸ì¤„ì—ì„œ ì—¬ì„¯ì§¸ì¤„ë¡œ ë³€í•˜ëŠ” ê³¼ì •ì„ í•œ ë²ˆ ë” ë°˜ë³µí•œë‹¤ê³  í•˜ë©´ ì¼ê³±ë²ˆì§¸ ì‹ì´ ë˜ê³ , ì´ê²ƒì„ ë¬´í•œíˆ ë°˜ë³µí•œë‹¤ê³  ìƒê°í•˜ë©´ ì—¬ëŸë²ˆì§¸ ì‹ì´ ëœë‹¤.
ê·¸ë¦¬ê³  ê¸‰ìˆ˜ì˜ ì •ì˜ë¥¼ ìƒê°í•˜ë©´ ì•„í™‰ë²ˆì§¸ ì‹ì´ ì´ì™€ ê°™ê³ , ê·¸ê²ƒì€ ê³§ ì—´ë²ˆì§¸ ì‹ì˜ $v_{\pi'}(s)$ì˜ ì •ì˜ì™€ ì •í™•íˆ ì¼ì¹˜í•œë‹¤.
ì´ë ‡ê²Œ í•˜ì—¬ policy improvement theoremì´ ì¦ëª…ë˜ì—ˆë‹¤. $\square$

ì •ì±…ê°œì„ ì •ë¦¬ëŠ” ì •ì±… $\pi$ê°€ ì£¼ì–´ì ¸ ìˆì„ ë•Œ ì´ë³´ë‹¤ ë” ì¢‹ì€ ì •ì±… $\pi'$ì„ ë§Œë“¤ì–´ë‚¼ ì—¬ì§€ë¥¼ ë§ˆë ¨í•œë‹¤.
ê·¸ëŸ¬ë©´, ì´ ì •ë¦¬ë¥¼ ìµœëŒ€í•œìœ¼ë¡œ í™œìš©í•˜ì—¬ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì •ì±… $\pi'$ì´ ìˆë‹¤.
ì´ê²ƒì€ ë¶€ë“±ì‹ (4.7)ì˜ ì¢Œë³€ì´ ìµœëŒ€í•œìœ¼ë¡œ ë˜ëŠ” ê²½ìš°ì´ë‹¤.
ì´ ì •ì±…ì„ greedy policyë¼ê³  ë¶€ë¥¸ë‹¤.

greedy policy $\pi'$ëŠ” í†µìƒ ëª¨ë“  $s$ì— ëŒ€í•˜ì—¬

$$
\begin{align*}
\pi'(s)
&=\text{arg}\max_a q_\pi(s,a)\\
&=\text{arg}\max_a\mathbb E\left[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t=s,A_t=a\right]\tag{4.9}\\
&=\text{arg}\max_a\sum_{s',r}p(s',r\vert s,a)\left[r+\gamma v_\pi(s')\right]
\end{align*}
$$

ì¸ ì •ì±… $\pi'$ìœ¼ë¡œ ì •ì˜í•œë‹¤.
ì¦‰, ì—¬ëŸ¬ $a$ë“¤ ì¤‘ $q_\pi(s,a)$ê°€ ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” $a$ë¥¼ $\pi'(s)$ë¡œ ì •ì˜í•˜ëŠ” ê²ƒì´ë‹¤.
ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” $a$ëŠ” ìœ ì¼í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëŸ¬í•œ $a$ë“¤ ì¤‘ í•˜ë‚˜ë¥¼ $\pi'(a)$ë¡œ ì •ì˜í•œë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.

ì´ëŸ¬í•œ $\pi'$ëŠ” deterministic policyì´ë‹¤.
ë°˜ë©´, $\pi'$ë¥¼ stochastic policyë¡œì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•´ë„ ë˜ê² ë‹¤.
ì£¼ì–´ì§„ $s$ì— ëŒ€í•´ì„œ $q$ê°’ì´ ê°€ì¥ í° action(ë“¤)ì„ ë‹¤ ë”í•´ì„œ 1ì´ ë˜ë„ë¡ í™•ë¥ ì„ ì£¼ê³  ë‚˜ë¨¸ì§€ actionì— ëŒ€í•´ì„œëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì£¼ëŠ” ê²ƒì´ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, $A_s=\\{a'\in A:q_\pi(s,a')=\max_{a\in\mathcal A}q_\pi(s,a)\\}$ ë¡œ ë‘ê³ 

$$
\begin{align*}
\pi'(a|s) =
\begin{cases}
\frac1{\left|A_s\right|}&a\in A_s\\
0                   &a\notin A_s\\
\end{cases}
\end{align*}
$$

ì™€ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

<div class="notice--info">
<b> policy improvement (ì •ì±… ê°œì„ ) </b> <br>
ì •ì±… $\pi$ì— ëŒ€í•˜ì—¬ $\pi'\ge\pi$ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì •ì±… $\pi'$ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì„ <b> ì •ì±…ê°œì„  </b>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.
greedy policyëŠ” ì •ì±…ê°œì„  ì¤‘ ê°€ì¥ ì ê·¹ì ì¸ ì •ì±…ê°œì„ ì´ë‹¤.
</div>

ì£¼ì–´ì§„ ì •ì±… $\pi$ì— ëŒ€í•œ greedy policy $\pi'$ê°€ ë”ì´ìƒ ê°œì„ ë˜ì§€ ì•Šê³  ì´ì „ê³¼ ê·¸ ê°€ì¹˜ê°€ ê°™ë‹¤ê³ , ì¦‰ $v_{\pi'}=v_\pi$ë¥¼ ë§Œì¡±í•œë‹¤ê³  í•˜ì.
ê·¸ëŸ¬ë©´ (4.9)ì— ì˜í•´

$$
\begin{align*}
v_{\pi'}(s)
&=q_{\pi'}(s,\pi'(s))\\
&=q_\pi(s,\pi'(s))\\
&=\max_a q_\pi(s,a)\\
&=\max_a\sum_{s',r}p(s',r\vert s,a)\left[r+\gamma v_\pi(s')\right]\\
&=\max_a\sum_{s',r}p(s',r\vert s,a)\left[r+\gamma v_{\pi'}(s')\right]
\end{align*}
$$

ì´ë‹¤.
(ì´ê²ƒì€ Sutton ì±…ì— ì¨ì§„ ê²ƒì„ ë” ì„¸ë¶€ì ìœ¼ë¡œ ì¨ë³¸ ê²ƒì´ë‹¤.)
ì²«ë²ˆì§¸ ì¤„ì€ ê¸°í˜¸ì˜ ì •ì˜ì— ì˜í•´ ë‹¹ì—°íˆ ì„±ë¦½í•œë‹¤.
ë‘ë²ˆì§¸ ì¤„ì€ $v_{\pi'}=v_\pi$ë¡œë¶€í„° $q_{\pi'}=q_\pi$ë¥¼ ìœ ë„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê·¸ë ‡ë‹¤(Bellman equation).
ì„¸ë²ˆì§¸ì™€ ë„¤ë²ˆì§¸ ì¤„ì€ (4.9)ì—ì„œë¶€í„° ë‚˜ì™”ê³ , ë§ˆì§€ë§‰ ë‹¤ì„¯ë²ˆì§¸ ì¤„ì€ $v_{\pi'}=v_\pi$ì´ë‹¤.

ê·¸ëŸ¬ë©´ ìœ„ ì‹ì€ Bellman optimal equation (3.18)ê³¼ ê°™ë‹¤.
íŠ¹ë³„í•œ ê²½ìš°ê°€ ì•„ë‹Œ ì´ìƒ ì—°ë¦½ë°©ì •ì‹ Bellman optimal equationì˜ í•´ê°€ ìœ ì¼í•  ê²ƒì´ë¯€ë¡œ, ì´ ì •ì±… $\pi'$ì€ optimal policyë¼ê³  ë§í•  ìˆ˜ ìˆë‹¤.

## 4.3 Policy Iteration

ì–´ë–¤ $\pi$ì— ëŒ€í•˜ì—¬ policy evaluationê³¼ policy improvementë¥¼ ë°˜ë³µí•  ìˆ˜ ìˆë‹¤.

![policy iteration]({{site.url}}\images\2025-09-18-dynamic_programming\policy_iteration.png){: .img-80-center}

ì •ì±… í‰ê°€ì™€ ì •ì±… ê°œì„ ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì¸ë° ì´ê²ƒì„ policy iterationì´ë¼ê³  í•œë‹¤.
ì •ì±…ê°œì„ ì‹œ deterministicí•œ policyë¥¼ íƒí•œë‹¤ê³  ê°€ì •í•˜ë©´, finite MDPì˜ deterministic policyì˜ ê°œìˆ˜ëŠ” ìœ ì¼í•˜ê³ , optimal policy ë˜í•œ deterministicí•˜ë‹¤ê³  ê°€ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ policy iterationëŠ” ì–¸ì  ê°€ ëë‚œë‹¤.
ì¦‰, ì–´ëŠ ìˆœê°„ ($k$ë²ˆì§¸ì—) ìµœì ì •ì±… $\pi_\ast$ì— ë„ë‹¬í•˜ë©° ê·¸ë•Œì˜ ì •ì±…ì€ $v_{\pi_k}=v_{\pi_{k-1}}$ë¥¼ ë§Œì¡±í•  ê²ƒì´ë‹¤.

ì •ì±…í‰ê°€ì‹œ ê°€ì¹˜í•¨ìˆ˜ì˜ ì´ˆê¹ƒê°’ì„ ì„¤ì •í•´ì•¼ í–ˆë‹¤.
ê·¸ ì´ˆê¹ƒê°’ì´ ì–´ë–»ê±´ í•­ìƒ $v_\pi$ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì€ ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ì¦ëª…í–ˆì—ˆë‹¤.
í•˜ì§€ë§Œ ì´ ê²½ìš°ì—ëŠ” ì´ˆê¹ƒê°’ì„ ì§ì „ ê°€ì¹˜í•¨ìˆ˜ë¡œ ë‘ë©´ ìˆ˜ë ´ ì†ë„ì— ìˆì–´ì„œ ìœ ë¦¬í•˜ë‹¤ëŠ” ê²ƒì´ ì–¸ê¸‰ë˜ê³  ìˆë‹¤.
ì¦‰, $\pi_i\stackrel{E}{\longrightarrow}v_{\pi_i}$ì˜ ì •ì±…í‰ê°€ë¥¼ ì§„í–‰í•  ë•Œ ê°€ì¹˜í•¨ìˆ˜ì˜ ì´ˆê¹ƒê°’ì„ $v_{i-1}$ë¡œ ë†“ìœ¼ë©´ ìˆ˜ë ´ì´ ë¹¨ë¼ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.
ì´ê²ƒì„ warm startë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

ì•„ë˜ëŠ” Sutton ì±…ì— ì‹¤ë¦° pseudocodeì´ë‹¤.
warm start ë²„ì „ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![pseudocode : policy iteration]({{site.url}}\images\2025-09-18-dynamic_programming\pseudocode-policy_iteration.png){: .img-80-center}

### 4.3.1 ì½”ë“œ êµ¬í˜„

ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ grid worldì—ì„œ equiprobable policyì— ëŒ€í•œ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•´ë´¤ì—ˆë‹¤.

![grid world]({{site.url}}\images\2025-09-18-dynamic_programming\grid_world.png){: .img-40-center}

ì´ë²ˆì—ëŠ” policy iterationì„ í†µí•´ ìµœì  ì •ì±…ì„ ì°¾ì•„ë³´ë ¤ í•œë‹¤.
4.5ì¥ì—ëŠ” policy evaluationì˜ ë¹ˆë„ì— ë”°ë¼ syncronous PIì™€ syncronous PIë¥¼ êµ¬ë¶„í•˜ê³  ìˆëŠ”ë° ì´ê²ƒì„ `in_place` ì¸ìë¡œ ë‘ì—ˆë‹¤. (asyncronous PI : `in_place=True`)
ë˜, ì •ì±…í‰ê°€ëŠ” ì •ì±… í‰ê°€ì˜ ê³¼ì • (í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸, sweep)ì´ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë˜ëŠ”ë° ëª¨ë“  sweep ì´í›„ì— policy improvementë¡œ ë„˜ì–´ê°ˆì§€ ì•„ë‹ˆë©´ í•œ ë²ˆì˜ sweepë§ˆë‹¤ policy improvementë¡œ ë„˜ì–´ê°ˆì§€ ê²°ì •í•˜ëŠ” ì¸ìë¡œ `max_sweep`ì„ ë„ì…í–ˆë‹¤. (ëª¨ë“  sweep ì´í›„ì— PIë¡œ ë„˜ì–´ê° : `max_sweep=True`)

```
from env import GridWorld
env = GridWorld()
V_init = {s: 0.0 for s in env.get_states()}  # Value function
policy_init = {s: 'R' for s in env.get_states()}  # Initial policy

def policy_evaluation(policy, V, in_place, max_sweep, threshold=0.001):
    """
    Policy Evaluation
    
    Args:
        in_place (bool) : True = asyncronous update, False : syncronous update
        max_sweep (bool) : True = sweeps until convergence, False : one sweep
        threshold (float) : convergence threshold
    """
    while True:
        
        if not in_place:
            V_old = V.copy()

        delta = 0
        for state in env.get_states():
            if env.is_terminal(state):
                continue
            
            action = policy[state]
            next_state = env.get_next_state(state, action)
            reward = env.get_reward(state, action)

            if in_place:
                # Asynchronous: ì¦‰ì‹œ ì—…ë°ì´íŠ¸ (í˜„ì¬ V ì‚¬ìš©)
                v_old = V[state]
                V[state] = reward + env.gamma * V[next_state]
            else:
                # Synchronous: old V ì‚¬ìš©í•´ì„œ new V ê³„ì‚°
                v_old = V_old[state]
                V[state] = reward + env.gamma * V_old[next_state]

            delta = max(delta, abs(v_old - V[state]))
                
        # ì¢…ë£Œ ì¡°ê±´
        if max_sweep:
            if delta < threshold:
                break
        else:
            break
    return V

def policy_improvement(policy, V):
    """Policy Improvement"""
    is_convergent = True
    
    for state in env.get_states():
        if env.is_terminal(state):
            continue
        
        old_action = policy[state]
        
        # ëª¨ë“  í–‰ë™ì— ëŒ€í•´ Qê°’ ê³„ì‚°
        action_values = {}
        for action in env.actions:
            next_state = env.get_next_state(state, action)
            reward = env.get_reward(state, action)
            action_values[action] = reward + env.gamma * V[next_state]
        
        # ìµœì„ ì˜ í–‰ë™ ì„ íƒ
        policy[state] = max(action_values, key=action_values.get)
        
        if old_action != policy[state]:
            is_convergent = False
    
    return policy, is_convergent


def print_results(policy, V):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\nValue Function:")
    for r in range(env.rows - 1, -1, -1):  # 2, 1, 0 ìˆœì„œ (ìƒí•˜ë°˜ì „)
        row_values = []
        for c in range(env.cols):
            if (r, c) == env.wall:
                row_values.append("  WALL ")
            else:
                row_values.append(f"{V[(r,c)]:6.2f}")
        print("  ".join(row_values))
    
    print("Policy:")
    for r in range(env.rows - 1, -1, -1):  # 2, 1, 0 ìˆœì„œ (ìƒí•˜ë°˜ì „)
        row_policy = []
        for c in range(env.cols):
            if (r, c) == env.wall:
                row_policy.append(" W ")
            elif env.is_terminal((r, c)):
                row_policy.append(" T ")
            else:
                row_policy.append(f" {policy[(r,c)]} ")
        print("  ".join(row_policy))


def policy_iteration(policy, V, in_place, max_sweep):
    """Policy Iteration"""
    iteration = 0

    print(f"\n{'='*60}")
    print(f"ğŸ¯ POLICY ITERATION  â”‚  {"Asyncronous" if in_place else "Syncronous"} + {"Full sweep" if max_sweep else "single sweep"}")
    print(f"{'='*60}\n")
    
    while True:
        iteration += 1
        
        # Iterationì€ ê°„ì†Œí™”
        print(f"\n[Iteration {iteration}]")
        
        V = policy_evaluation(policy, V, in_place=in_place, max_sweep=max_sweep)
        policy, is_convergent = policy_improvement(policy, V)
        
        print_results(policy, V)
        
        if is_convergent:
            print(f"\nâœ… Policy converged after {iteration} iterations!")
            break

```

### 4.3.2 êµ¬í˜„ ê²°ê³¼

ì˜µì…˜ì´ ë‘ ê°œì´ë‹ˆ ì´ ë„¤ ë²ˆì˜ ì‹¤í—˜ì„ í•´ë³¼ ìˆ˜ ìˆë‹¤.
ê·¸ë•Œë§ˆë‹¤ì˜ ê²°ê³¼ë¥¼ í‘œë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

|                   | `in_place=True` | `in_place=False` |
|:-----------------:|:---------------:|:----------------:|
| `max_sweep=True`  |1ë²ˆë§Œì— ìˆ˜ë ´       |4ë²ˆë§Œì— ìˆ˜ë ´|
| `max_sweep=False` |1ë²ˆë§Œì— ìˆ˜ë ´       |1ë²ˆë§Œì— ìˆ˜ë ´|

ì¦‰, 4.3ì˜ ë³¸ë¬¸ì—ì„œ ì„¤ëª…í•œ ê¸°ë³¸ì ì¸ policy iteration ë°©ì‹ì€ `in_place=False`, `max_sweep=True`ì¸ë° ì´ ê²½ìš°ì—ëŠ” 4ë²ˆì˜ iterationì„ ê±°ì³ì„œ ì •ì±…ì´ ìˆ˜ë ´í–ˆë‹¤.
ë°˜ë©´, `in_place=True`ë¡œ ë‘ì–´ asyncronous ë°©ì‹ì„ ì·¨í•˜ê±°ë‚˜ ì•„ë‹ˆë©´ sweepë§ˆë‹¤ policy improvementë¥¼ ì‹œí–‰í•œ ê²½ìš°ì—ëŠ” í•œ ë²ˆë§Œì— ì •ì±…ì´ ìˆ˜ë ´í–ˆë‹¤.

ì•„ë˜ëŠ” ì„¸ë¶€ ê²°ê³¼ì´ë‹¤.
```
# syncronous, full sweep
policy_iteration(policy_init, V_init, in_place=True, max_sweep=True)
============================================================
ğŸ¯ POLICY ITERATION  â”‚  Asyncronous + Full sweep
============================================================


[Iteration 1]

Value Function:
  0.62    0.80    1.00    0.00
 -0.99    WALL    -1.00    0.00
 -0.99   -0.99   -0.99   -0.99
Policy:
 R    R    R    T 
 U    W    U    T 
 U    U    D    D 

[Iteration 2]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31   -0.99   -0.99   -0.99
Policy:
 R    R    R    T 
 U    W    U    T 
 U    L    U    D 

[Iteration 3]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31    0.18    0.62   -0.99
Policy:
 R    R    R    T 
 U    W    U    T 
 U    R    U    L 

[Iteration 4]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31    0.46    0.62    0.46
Policy:
 R    R    R    T 
 U    W    U    T 
 U    R    U    L 

âœ… Policy converged after 4 iterations!
# asyncronous, full sweep
policy_iteration(policy_init, V_init, in_place=False, max_sweep=True)
============================================================
ğŸ¯ POLICY ITERATION  â”‚  Syncronous + Full sweep
============================================================


[Iteration 1]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31    0.46    0.62    0.46
Policy:
 R    R    R    T 
 U    W    U    T 
 U    R    U    L 

âœ… Policy converged after 1 iterations!
# syncronous, single sweep
policy_iteration(policy_init, V_init, in_place=True, max_sweep=False)
============================================================
ğŸ¯ POLICY ITERATION  â”‚  Asyncronous + single sweep
============================================================


[Iteration 1]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31    0.46    0.62    0.46
Policy:
 R    R    R    T 
 U    W    U    T 
 U    R    U    L 

âœ… Policy converged after 1 iterations!
# asyncronous, single sweep
policy_iteration(policy_init, V_init, in_place=False, max_sweep=False)
============================================================
ğŸ¯ POLICY ITERATION  â”‚  Syncronous + single sweep
============================================================


[Iteration 1]

Value Function:
  0.62    0.80    1.00    0.00
  0.46    WALL     0.80    0.00
  0.31    0.46    0.62    0.46
Policy:
 R    R    R    T 
 U    W    U    T 
 U    R    U    L 

âœ… Policy converged after 1 iterations!
```

## 4.4 Value Iteration

policy iterationì€ ì •ì±…í‰ê°€ì™€ ì •ì±…ê°œì„ ì˜ ë°˜ë³µì´ì—ˆì§€ë§Œ, ì •ì±…ê°œì„ ë„ ê·¸ ìì²´ë¡œ ë°˜ë³µ ì•Œê³ ë¦¬ì¦˜ì´ì—ˆë‹¤.
ì •ì±…ê°œì„  í•œ ë²ˆë‹¹ ì •ì±…í‰ê°€ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°ì—ëŠ” ì¡°ê¸ˆ ì§€ë£¨í•  ìˆ˜ ìˆë‹¤.
ê·¸ëŸ¬ë‹ˆ, ì •ì±…ê°œì„  í•œ ë²ˆë‹¹ ì •ì±…í‰ê°€ì˜ ê³¼ì •(sweep)ì„ í•œ ë²ˆì”© ìˆ˜í–‰í•˜ë©´ ì´ ë•Œì—ë„ ìµœì ì •ì±…ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì´ ì•Œë ¤ì ¸ ìˆìœ¼ë©° ì´ ê³¼ì •ì„ value iterationì´ë¼ê³  í•œë‹¤ê³  Suttonì€ ì“°ê³  ìˆë‹¤.
í•˜ì§€ë§Œ ì´ê±´ ë¹„ìœ ì ì¸ í‘œí˜„ì´ë¼ê³  ë´ì•¼ í•  ê²ƒ ê°™ë‹¤.
ì‹¤ì œë¡œ sweepê³¼ PIë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì ìš©í•˜ë©´ ì•„ë˜ ì‹ì´ ë‚˜ì˜¤ì§€ëŠ” ì•ŠëŠ”ë‹¤.

value iterationì´ë€ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ ì í™”ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ë‚˜ê°€ëŠ” ë°©ì‹ì„ ë§í•œë‹¤.

$$
\begin{align*}
v_{k+1}(s)
&=\max_a\mathbb E\left[R_{t+1}+\gamma v_k(S_{t+1})\vert S_t=s, A_t=a\right]\\
&=\max_a\sum_{s',r}p(s',r|s,a)\left[r+v_k(s')\right]\tag{4.10}
\end{align*}
$$

ìœ„ì˜ ì‹ê³¼ ì•„ë˜ ì‹ì´ ê°™ë‹¤ëŠ” ê²ƒì€ ë‹¤ìŒ ì‹ì—ì„œ ëª…ë°±í•˜ë‹¤.
<!-- ê¸°ë³¸ì ìœ¼ë¡œ (4.3)ê³¼ (4.4)ì´ ê°™ë‹¤ëŠ” ì´ì „ í¬ìŠ¤íŠ¸ì˜ ì¦ëª…ê³¼ ê±°ì˜ ê°™ë‹¤. -->

$$
\begin{align*}
&\mathbb E\left[R_{t+1}+\gamma v_k(S_{t+1})\vert S_t=s, A_t=a\right]\\
=&\sum_{s',r}p(s',r\vert s,a)\mathbb E\left[R_{t+1}+\gamma v_k(S_{t+1})\vert S_t=s,A_t=a,R_{t+1}=r,S_{t+1}=s'\right]\\
=&\sum_{s',r}p(s',r\vert s,a)\left[r+\gamma v_k(s')\right]
\end{align*}
$$

value iterationì€ policy iterationê³¼ëŠ” ë§ì´ ë‹¤ë¥´ê²Œ ì •ì±…í‰ê°€ì™€ ì •ì±…ê°œì„ ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ì…ˆì´ê³  ê·¸ëŸ° ì ì—ì„œ ë‚˜ì¤‘ì— ë‚˜ì˜¤ëŠ” Q-learningê³¼ ë¹„ìŠ·í•˜ë‹¤.

value iterationì„ í†µí•´ optimal value function $v_\ast$ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œ operator $\bar{\mathcal T}$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì.
[Dawei Li, Zikun Yeì˜ ìë£Œ](http://127.0.0.1:4000/data-science/policy_improvement/#44-value-iteration)ë¥¼ ì°¸ê³ í•˜ì˜€ë‹¤.

$$
\begin{align*}
\bar{\mathcal T}v(s)
&=\max_a\sum_{s',r}p(s',r\vert s,a)\left[r+v_\pi(s')\right]\\
&=\max_a\mathbb E\left[R_{t+1}+v(S_{t+1})\vert S_t=a, A_t=a\right]\\
\end{align*}
$$

ë¨¼ì € ì–¸ê¸‰í•  ê²ƒì€ $\bar{\mathcal T}$ì˜ ì •ì˜ì— ë”°ë¥´ë©´ $v$ì— ëŒ€í•œ Bellman optimal equationì´

$$v_\ast(s)=(\mathcal Tv_\ast)(s)$$

ë¡œ ì“°ì—¬ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤.
ë˜í•œ, Bellman optimal equationì´ ì‹ì˜ ê°œìˆ˜ì™€ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ê°™ì€ (ë¹„ì„ í˜•) ì—°ë¦½ë°©ì •ì‹ì´ë¯€ë¡œ íŠ¹ìˆ˜í•œ ìƒí™©ì´ ì•„ë‹Œ ì´ìƒì€ í•´ê°€ í•˜ë‚˜ì´ê³ , ë”°ë¼ì„œ $v=(\bar{\mathcal T}v)(s)$ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ì–´ë–¤ $v$ê°€ ìˆë‹¤ë©´ ê·¸ $v$ëŠ” optimal value functionì´ë¼ëŠ” ê²ƒë„ ì•Œ ìˆ˜ ìˆë‹¤.

ì¦ëª…ì— ì•ì„œ ì ˆëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— ê´€í•œ ê°„ë‹¨í•œ ë‹¤ìŒ ì‹ $(\ast)$ì„ ì°¸ê³ í•˜ì.

$$
\begin{aligned}
\left|\max_af(a)-\max_ag(a)\right|
&=\max_af(a)-\max_ag(a)\\
&=f(a_\ast)-\max_ag(a)\\
&\le f(a_\ast)-g(a_\ast)\\
&\le\max_a\left|f(a)-g(a)\right|\\
\end{aligned}
\tag{$\ast$}
$$

ì´ë•Œ, ì¼ë°˜ì„±ì„ ìƒì§€ ì•Šê³  $\max_a f(a)\ge \max_ag(a)$ë¡œ ë‘ì—ˆê³  $a_\ast=\text{arg}\max_af(a)$ë¡œ ë‘ì—ˆë‹¤.

ë¨¼ì €, $\bar{\mathcal T}$ê°€ contraction mappingì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.
ì„ì˜ì˜ $v$, $w$ì— ëŒ€í•˜ì—¬ ì‹ (4.10)ì— ì˜í•´ ì •ì˜ëœ ì í™”ì‹ì¸

$$
\begin{align*}
\left|(\bar{\mathcal T^\pi v})(s)-(\bar{\mathcal T^\pi w})(s)\right|
&=\left|
    \max_a\sum_{s',r}p(s',r\vert s,a)\left[r+v(s')\right]
    -\max_a\sum_{s',r}p(s',r\vert s,a)\left[r+w(s')\right]
\right|\\
&\stackrel{(\ast)}{\le}
\max_a\left|
    \sum_{s',r}p(s',r\vert s,a)\left[r+v(s')\right]
    -\sum_{s',r}p(s',r\vert s,a)\left[r+w(s')\right]
\right|\\
&=\gamma\max_a\sum_{s',r}p(s',r\vert s,a)\left|v(s')-w(s')\right|\\
&=\gamma\sum_{s',r}p(s',r\vert s,a_\ast)\left|v(s')-w(s')\right|\\
&=\gamma\mathbb E\left[
    \left|v(S_{t+1})-w(S_{t+1}))\right|\;\vert S_t=s, A_t=a_\ast
    \right]\\
&\le\gamma\max_{s'}|v(s')-w(s')|\\
&=\gamma\left\Vert v-w\right\Vert_{\infty}
\end{align*}
$$

ì´ë‹¤.
ì²«ë²ˆì§¸ ì¤„ì€ ê¸°í˜¸ì˜ ì •ì˜, ë‘ë²ˆì§¸ ì¤„ì€ $(\ast)$, ì„¸ë²ˆì§¸ ì¤„ì€ ë‹¨ìˆœ ê³„ì‚°ì´ë‹¤.
ë„¤ë²ˆì§¸ ì¤„ì—ì„œëŠ” ì„¸ë²ˆì§¸ ì¤„ì˜ argmaxë¥¼ $a_\ast$ë¡œ ì¡ì€ ê²ƒì´ê³  ë‹¤ì„¯ë²ˆì§¸ ì¤„ì€ ê¸°í˜¸ì˜ ì •ì˜ì— ì˜í•´ ë‹¹ì—°í•˜ë‹¤.
ì—¬ì„¯ë²ˆì§¸ ì¤„ì€ $A_t=a_\ast$ì¸ ìƒí™©ë³´ë‹¤ë„ ë” í¬ê²Œ ìµœëŒ“ê°’ì„ ì¡ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë©°, ì¼ê³±ë²ˆì§¸ ì¤„ì€ ê¸°í˜¸ì˜ ì •ì˜ë¡œë¶€í„° ë‹¹ì—°í•˜ë‹¤.

ì´ì œ ì¢Œë³€ì— $\Vert\cdot\Vert_\infty$ë¥¼ ì·¨í•˜ë©´

$$
\left\Vert\bar{\mathcal T^\pi v}-\bar{\mathcal T^\pi w}\right\Vert_\infty
\le\gamma\Vert v-w\Vert_\infty
$$

ì´ë‹¤.
ë”°ë¼ì„œ $\mathcal T^\pi$ëŠ” contraction mappingì´ë‹¤.
ê·¸ëŸ¬ë©´ contraction principleì— ì˜í•´, ì„ì˜ì˜ $v_0:\mathcal S\to\mathbb R$ì— ëŒ€í•˜ì—¬ ì í™”ì‹ (4.10) í˜¹ì€

$$
v_{k+1}=\bar{\mathcal T}v_k
$$

ì— ì˜í•´ ì •ì˜ëœ ìˆ˜ì—´ $\\{v_k\\}_{k=0}^\infty$ëŠ” ìˆ˜ë ´í•œë‹¤.

ì¡°ê¸ˆ ë” ì •í™•í•˜ê²ŒëŠ”, ì–´ë–¤ $K$ì— ëŒ€í•˜ì—¬ $v_{K+1}=v_K$ê°€ ë˜ëŠ”ë° ê·¸ëŸ¬ë©´
$v_K=\bar{\mathcal T}v_K$ì´ ë˜ì–´ $v_K$ê°€ $\bar{\mathcal T}$ì˜ fixed pointê°€ ëœë‹¤.
ê·¸ëŸ°ë° $\bar{\mathcal T}$ì˜ ìœ ì¼í•œ ê³ ì •ì ì€ $v_\ast$ì´ë¯€ë¡œ $v_K=v_\ast$ì´ë‹¤.

$$
\lim_{k\to\infty} v_k=v_\ast
$$

ì´ë‹¤.
ì—¬ê¸°ì„œ ê·¹í•œì€ $\Vert\cdot\Vert_\infty$ì˜ ê´€ì ì—ì„œì˜ ê·¹í•œì´ë‹¤.